{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDkpircCGcpBLs6tbu+I7w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs-iuu/ocr-2025-fall-cv/blob/main/notebooks/12.evaluation_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Evaulation Metrics"
      ],
      "metadata": {
        "id": "cjvBkHmZm311"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_l1eTTlm3GN",
        "outputId": "ca17c777-cea9-47a9-c197-4e4bf7cdfd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Data Setup: Multi-Class Imbalance Simulation ---\n",
            "Total Samples: 100\n",
            "Classes found: [0 1 2]\n",
            "Class Support (Imbalance): {np.int64(0): 80, np.int64(1): 15, np.int64(2): 5}\n",
            "\n",
            "--- Per-Class Metrics ---\n",
            "| Class      |   Support |      TP |     FP |     FN |   Precision |   Recall |   F1-Score |\n",
            "|:-----------|----------:|--------:|-------:|-------:|------------:|---------:|-----------:|\n",
            "| А (Common) |   80.0000 | 80.0000 | 9.0000 | 0.0000 |      0.8989 |   1.0000 |     0.9467 |\n",
            "| Б (Medium) |   15.0000 | 11.0000 | 0.0000 | 4.0000 |      1.0000 |   0.7333 |     0.8462 |\n",
            "| Щ (Rare)   |    5.0000 |  0.0000 | 0.0000 | 5.0000 |      0.0000 |   0.0000 |     0.0000 |\n",
            "\n",
            "====================================\n",
            "OVERALL ACCURACY = 0.9100 (91/100)\n",
            "FINAL RESULT: MACRO F1-SCORE = 0.5976\n",
            "====================================\n",
            "\n",
            "[Discussion Prompts]:\n",
            "1. Compare the Macro F1-Score to the Overall Accuracy. Why is the Macro F1-Score significantly lower?\n",
            "2. Look at the metrics for 'Щ' (Class 2). Why is its Recall 0.0 and its Precision 0.0?\n",
            "3. Explain why Macro F1-Score is the superior evaluation metric for this imbalanced OCR problem.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pandas as pd # Import pandas for nice table display (optional, but helpful in a notebook)\n",
        "\n",
        "# --- CELL 1: Setup and Data Simulation ---\n",
        "\n",
        "print(\"--- 1. Data Setup: Multi-Class Imbalance Simulation ---\")\n",
        "\n",
        "# We are simulating a classification task for 3 Cyrillic characters:\n",
        "# Class 0: 'А' (Common) - Support: 80\n",
        "# Class 1: 'Б' (Medium) - Support: 15\n",
        "# Class 2: 'Щ' (Rare) - Support: 5\n",
        "\n",
        "# Total Samples: 100\n",
        "\n",
        "# Ground Truth (Actual) Labels\n",
        "Y_TRUE = np.array([\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 20 A's\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 40 A's\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 60 A's\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 80 A's\n",
        "    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, # 10 B's\n",
        "    1, 1, 1, 1, 1, # 15 B's\n",
        "    2, 2, 2, 2, 2  # 5 Щ's (Shcha)\n",
        "])\n",
        "\n",
        "# Model Predictions (Simulated to be good on common, bad on rare)\n",
        "Y_PRED = np.array([\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 20 A's\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 40 A's\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 60 A's\n",
        "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, # 80 A's (75 correct predictions, 5 FPs on other classes)\n",
        "\n",
        "    # Mistakes on 'Б' (Class 1) - 12 correct, 3 missed (FNs)\n",
        "    1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
        "    1, 1, 0, 0, 0,\n",
        "\n",
        "    # Mistakes on 'Щ' (Class 2) - 0 correct, 5 missed (FNs)\n",
        "    0, 0, 0, 0, 0\n",
        "])\n",
        "\n",
        "\n",
        "# --- Display Summary ---\n",
        "CLASSES = np.unique(Y_TRUE)\n",
        "print(f\"Total Samples: {len(Y_TRUE)}\")\n",
        "print(f\"Classes found: {CLASSES}\")\n",
        "counts = defaultdict(int)\n",
        "for label in Y_TRUE:\n",
        "    counts[label] += 1\n",
        "print(f\"Class Support (Imbalance): {dict(counts)}\")\n",
        "\n",
        "# --- CELL 2: Metric Functions - STUDENTS MUST COMPLETE TODOS ---\n",
        "\n",
        "def calculate_confusion_components(y_true, y_pred, target_class):\n",
        "    \"\"\"\n",
        "    Calculates the True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
        "    for a given target class (one-vs-rest approach).\n",
        "    \"\"\"\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "\n",
        "    for actual, predicted in zip(y_true, y_pred):\n",
        "        # TODO 1: Implement the logic to count TP, FP, and FN based on definitions:\n",
        "\n",
        "        # TP: Actual is TARGET_CLASS AND Predicted is TARGET_CLASS\n",
        "        if actual == target_class and predicted == target_class:\n",
        "            TP += 1\n",
        "        # FN: Actual is TARGET_CLASS AND Predicted is NOT TARGET_CLASS\n",
        "        elif actual == target_class and predicted != target_class:\n",
        "            FN += 1\n",
        "        # FP: Actual is NOT TARGET_CLASS AND Predicted is TARGET_CLASS\n",
        "        elif actual != target_class and predicted == target_class:\n",
        "            FP += 1\n",
        "\n",
        "    return TP, FP, FN\n",
        "\n",
        "\n",
        "def calculate_precision(TP, FP):\n",
        "    \"\"\"\n",
        "    Calculates Precision: TP / (TP + FP)\n",
        "    \"\"\"\n",
        "    # TODO 2: Calculate Precision, handling division by zero if (TP + FP) is zero.\n",
        "    if (TP + FP) == 0:\n",
        "        return 0.0 # If the model never predicted this class, precision is undefined (treat as 0)\n",
        "    return TP / (TP + FP)\n",
        "\n",
        "\n",
        "def calculate_recall(TP, FN):\n",
        "    \"\"\"\n",
        "    Calculates Recall: TP / (TP + FN)\n",
        "    \"\"\"\n",
        "    # TODO 3: Calculate Recall, handling division by zero if (TP + FN) is zero.\n",
        "    if (TP + FN) == 0:\n",
        "        return 0.0 # If the class had no actual samples, recall is undefined (treat as 0)\n",
        "    return TP / (TP + FN)\n",
        "\n",
        "\n",
        "def calculate_f1_score(precision, recall):\n",
        "    \"\"\"\n",
        "    Calculates F1-Score: 2 * (P * R) / (P + R) (Harmonic Mean)\n",
        "    \"\"\"\n",
        "    # TODO 4: Calculate F1-Score, handling division by zero if (Precision + Recall) is zero.\n",
        "    if (precision + recall) == 0:\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "# --- CELL 3: Multi-Class Aggregation Logic ---\n",
        "\n",
        "def calculate_macro_f1(y_true, y_pred, classes):\n",
        "    \"\"\"\n",
        "    Calculates the Macro F1-Score by averaging the F1-Scores of all classes.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    class_f1_scores = []\n",
        "\n",
        "    # Map class IDs to human-readable labels for display\n",
        "    class_map = {0: 'А (Common)', 1: 'Б (Medium)', 2: 'Щ (Rare)'}\n",
        "\n",
        "    # 1. Loop through each class label\n",
        "    for class_id in classes:\n",
        "        class_label = class_map.get(class_id, f'Class {class_id}')\n",
        "\n",
        "        # 2. Calculate the Confusion Matrix components for the current class\n",
        "        TP, FP, FN = calculate_confusion_components(y_true, y_pred, class_id)\n",
        "\n",
        "        # 3. Calculate Precision, Recall, and F1-Score for the current class\n",
        "        P = calculate_precision(TP, FP)\n",
        "        R = calculate_recall(TP, FN)\n",
        "        F1 = calculate_f1_score(P, R)\n",
        "\n",
        "        # Store results for table display\n",
        "        results.append({\n",
        "            'Class': class_label,\n",
        "            'Support': counts[class_id],\n",
        "            'TP': TP, 'FP': FP, 'FN': FN,\n",
        "            'Precision': P,\n",
        "            'Recall': R,\n",
        "            'F1-Score': F1\n",
        "        })\n",
        "        class_f1_scores.append(F1)\n",
        "\n",
        "    # Display results nicely\n",
        "    df = pd.DataFrame(results).set_index('Class')\n",
        "    print(\"\\n--- Per-Class Metrics ---\")\n",
        "    print(df.to_markdown(floatfmt=\".4f\"))\n",
        "\n",
        "    # 5. Calculate the Macro F1-Score (unweighted average of all F1-scores)\n",
        "    # TODO 5: Implement the final average calculation\n",
        "    if not class_f1_scores:\n",
        "        return 0.0\n",
        "    macro_f1 = sum(class_f1_scores) / len(class_f1_scores)\n",
        "\n",
        "    return macro_f1\n",
        "\n",
        "\n",
        "# --- CELL 4: Execution and Analysis ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Calculate and display the Macro F1 Score\n",
        "    final_macro_f1 = calculate_macro_f1(Y_TRUE, Y_PRED, CLASSES)\n",
        "\n",
        "    # 2. Calculate overall Accuracy (for comparison)\n",
        "    correct_predictions = np.sum(Y_TRUE == Y_PRED)\n",
        "    total_samples = len(Y_TRUE)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # 3. Print Final Results\n",
        "\n",
        "    print(\"\\n====================================\")\n",
        "    print(f\"OVERALL ACCURACY = {accuracy:.4f} ({correct_predictions}/{total_samples})\")\n",
        "    print(f\"FINAL RESULT: MACRO F1-SCORE = {final_macro_f1:.4f}\")\n",
        "    print(\"====================================\")\n",
        "\n",
        "    # --- Student Analysis Prompts ---\n",
        "    print(\"\\n[Discussion Prompts]:\")\n",
        "    print(\"1. Compare the Macro F1-Score to the Overall Accuracy. Why is the Macro F1-Score significantly lower?\")\n",
        "    print(\"2. Look at the metrics for 'Щ' (Class 2). Why is its Recall 0.0 and its Precision 0.0?\")\n",
        "    print(\"3. Explain why Macro F1-Score is the superior evaluation metric for this imbalanced OCR problem.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Frtexoztm7Cy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}